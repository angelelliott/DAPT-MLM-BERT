{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1I_5V150f9BifjdibboYv4GVUhb5o7q4l",
      "authorship_tag": "ABX9TyOOjzN0QOn9a07r6DaIjBp3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daliarod96/DAPT-MLM-BERT/blob/main/pre_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Transformers library\n",
        "# I'm using an old version because newer versions were not compatible with\n",
        "# the Docker container that I used to run this program\n",
        "!pip install transformers==2.3.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46QpOg0mYjbs",
        "outputId": "1642a478-c534-4d7a-de03-adb2a94678bd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==2.3.0\n",
            "  Downloading transformers-2.3.0-py3-none-any.whl (447 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.4/447.4 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from transformers==2.3.0) (1.22.4)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from transformers==2.3.0) (4.65.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.118-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==2.3.0) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==2.3.0) (2022.10.31)\n",
            "Collecting botocore<1.30.0,>=1.29.118\n",
            "  Downloading botocore-1.29.118-py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==2.3.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==2.3.0) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==2.3.0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==2.3.0) (2.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==2.3.0) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==2.3.0) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==2.3.0) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.118->boto3->transformers==2.3.0) (2.8.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=66a5648ca15a8d5c21b396af72030ff7b5e581950a66079011eb040ba5d32665\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, jmespath, botocore, s3transfer, boto3, transformers\n",
            "Successfully installed boto3-1.26.118 botocore-1.29.118 jmespath-1.0.1 s3transfer-0.6.0 sacremoses-0.0.53 sentencepiece-0.1.98 transformers-2.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries \n",
        "import pandas as pd \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers"
      ],
      "metadata": {
        "id": "RB82gPKZY73B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creat a custom Tweets Dataset class \n",
        "class TweetsDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings):\n",
        "    self.encodings = encodings\n",
        "  def __getitem__(self, idx):\n",
        "    return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "  def __len__(self):\n",
        "    return len(self.encodings['input_ids'])\n"
      ],
      "metadata": {
        "id": "3lLxtPF4ZAPf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and process dataset\n",
        "text = pd.read_csv(\"/content/drive/MyDrive/myModels/profanitydatasetprocessed\")\n",
        "text = text['content']\n",
        "text = text.dropna()\n",
        "text = text.astype('str')\n",
        "text = text.tolist()\n",
        "text = text[:100]\n"
      ],
      "metadata": {
        "id": "6mdsAT-4ZCZQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the tokenizer and load the pre-trained model\n",
        "\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\", do_lower_case=True)\n",
        "print(\"Tokenizer imported\")\n",
        "model = BertForMaskedLM.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
        "print(\"Model imported\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO9454rzZNtP",
        "outputId": "f6cb26e7-1e52-4ec4-a01f-8eaec008e20d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer imported\n",
            "Model imported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "au-H9SfxYKn4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f7b6545-0b9e-40cf-aa4d-4ac24c0090b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset\n",
            "dataloader\n",
            "device\n",
            "model to device\n"
          ]
        }
      ],
      "source": [
        "# tokenize text for training\n",
        "\n",
        "result = tokenizer.batch_encode_plus(text)\n",
        "chunk_size = 512\n",
        "\n",
        "def group_texts(examples):\n",
        "  # concatenate all texts\n",
        "  concatenated_examples = {'input_ids':[], 'token_type_ids':[], 'attention_mask':[]}\n",
        "  for i in range(len(examples['input_ids'])):\n",
        "    concatenated_examples['input_ids']+=examples['input_ids'][i]\n",
        "  for i in range(len(examples['token_type_ids'])):\n",
        "    concatenated_examples['token_type_ids']+=examples['token_type_ids'][i]\n",
        "  for i in range(len(examples['attention_mask'])):\n",
        "    concatenated_examples['attention_mask']+=examples['attention_mask'][i]\n",
        "\n",
        "  # compute length of concatenated texts\n",
        "  total_length = len(concatenated_examples['input_ids'])\n",
        "\n",
        "  # We drop the last chunk if it's smaller than chunk_size\n",
        "  total_length = (total_length // chunk_size) * chunk_size\n",
        "\n",
        "  # Split by chunks of max_len\n",
        "  result = {\n",
        "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "\n",
        "  # Create a new labels column\n",
        "  result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "\n",
        "\n",
        "  result['input_ids'] = torch.tensor(result['input_ids'])\n",
        "  result['token_type_ids'] = torch.tensor(result['token_type_ids'])\n",
        "  result['attention_mask'] = torch.tensor(result['attention_mask'])\n",
        "  result['labels'] = torch.tensor(result['labels'])\n",
        "\n",
        "  \n",
        "  return result\n",
        "\n",
        "inputs = group_texts(result)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "We want to create our mask. \n",
        "Each token that is not a special token has a 15% chance of being masked.\n",
        "We don't want to mask [CLS], [SEP], or padding tokens. \n",
        "Correspond to the numbers 4, 5, and 1.\n",
        "\"\"\"\n",
        "\n",
        "rand = torch.rand(inputs['input_ids'].shape)\n",
        "\n",
        "mask_arr = (rand < 0.15) * (inputs['input_ids'] != 4) * (inputs['input_ids'] != 5)* (inputs['input_ids'] != 1)\n",
        "\n",
        "# masked tokens\n",
        "\n",
        "selection = []\n",
        "\n",
        "for i in range(mask_arr.shape[0]):\n",
        "  selection.append(\n",
        "    torch.flatten(mask_arr[i].nonzero()).tolist()\n",
        "  )\n",
        "\n",
        "for i in range(mask_arr.shape[0]):\n",
        "  inputs['input_ids'][i,selection[i]] = 0\n",
        "\n",
        "dataset = TweetsDataset(inputs)\n",
        "\n",
        "# pass dataset into DataLoader\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 32, shuffle=True)\n",
        "\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "model.train()\n",
        "\n",
        "from torch.optim import AdamW\n",
        "\n",
        "#optimizer\n",
        "optim = AdamW(model.parameters(), lr=5e-5, weight_decay = 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  loop = tqdm(dataloader, leave=True)\n",
        "  for batch in loop:\n",
        "    optim.zero_grad()\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels= batch['labels'].to(device)\n",
        "    \n",
        "\n",
        "    outputs = model(input_ids, attention_mask=attention_mask,\n",
        "                   lm_labels=labels)\n",
        "    loss, prediction_scores = outputs[:2]\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    loop.set_description(f'Epoch {epoch}')\n",
        "    loop.set_postfix(loss=loss.item())"
      ],
      "metadata": {
        "id": "uYffV5faZ1yf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8747410-5724-42df-d484-ae9dd6b4e201"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]<ipython-input-3-f45e2a3e1b39>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "Epoch 0: 100%|██████████| 1/1 [00:03<00:00,  3.36s/it, loss=0.324]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory = \"/content/drive/MyDrive/myModels\"\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "model.save_pretrained(save_directory)"
      ],
      "metadata": {
        "id": "hDuzBY3hZ5mN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zY-kvUq1vkti"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}