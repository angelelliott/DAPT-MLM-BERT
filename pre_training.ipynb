{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1I_5V150f9BifjdibboYv4GVUhb5o7q4l",
      "authorship_tag": "ABX9TyOHVSe5TVs1XJ1q+gnCxPmz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daliarod96/DAPT-MLM-BERT/blob/main/pre_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Transformers library\n",
        "# I'm using an old version because newer versions were not compatible with\n",
        "# the Docker container that I used to run this program\n",
        "!pip install transformers==2.3.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46QpOg0mYjbs",
        "outputId": "9637a7d7-842f-4e19-d3cd-9f0cd94675cc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==2.3.0 in /usr/local/lib/python3.9/dist-packages (2.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from transformers==2.3.0) (4.65.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from transformers==2.3.0) (0.1.98)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==2.3.0) (2022.10.31)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.9/dist-packages (from transformers==2.3.0) (1.26.118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from transformers==2.3.0) (1.22.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.9/dist-packages (from transformers==2.3.0) (0.0.53)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==2.3.0) (2.27.1)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from boto3->transformers==2.3.0) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from boto3->transformers==2.3.0) (0.6.0)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.118 in /usr/local/lib/python3.9/dist-packages (from boto3->transformers==2.3.0) (1.29.118)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==2.3.0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==2.3.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==2.3.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==2.3.0) (3.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==2.3.0) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==2.3.0) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==2.3.0) (8.1.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.118->boto3->transformers==2.3.0) (2.8.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries \n",
        "import pandas as pd \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers"
      ],
      "metadata": {
        "id": "RB82gPKZY73B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creat a custom Tweets Dataset class \n",
        "class TweetsDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings):\n",
        "    self.encodings = encodings\n",
        "  def __getitem__(self, idx):\n",
        "    return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "  def __len__(self):\n",
        "    return len(self.encodings['input_ids'])\n"
      ],
      "metadata": {
        "id": "3lLxtPF4ZAPf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and process dataset\n",
        "# profanities dataset of 5.5 million tweets containing commonly used slurs in Spanish\n",
        "text = pd.read_csv(\"/content/drive/MyDrive/myModels/profanitydatasetprocessed\")\n",
        "text = text['content']\n",
        "text = text.dropna()\n",
        "text = text.astype('str')\n",
        "text = text.tolist()\n",
        "\n",
        "#I'm just using 1000 samples to expedite processing \n",
        "text = text[:100]\n"
      ],
      "metadata": {
        "id": "6mdsAT-4ZCZQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the tokenizer and load the pre-trained model\n",
        "\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\", do_lower_case=True)\n",
        "model = BertForMaskedLM.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")"
      ],
      "metadata": {
        "id": "GO9454rzZNtP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "au-H9SfxYKn4"
      },
      "outputs": [],
      "source": [
        "# tokenize text for training\n",
        "\n",
        "result = tokenizer.batch_encode_plus(text)\n",
        "chunk_size = 128\n",
        "\n",
        "def group_texts(examples):\n",
        "  # concatenate all texts\n",
        "  concatenated_examples = {'input_ids':[], 'token_type_ids':[], 'attention_mask':[]}\n",
        "  for i in range(len(examples['input_ids'])):\n",
        "    concatenated_examples['input_ids']+=examples['input_ids'][i]\n",
        "  for i in range(len(examples['token_type_ids'])):\n",
        "    concatenated_examples['token_type_ids']+=examples['token_type_ids'][i]\n",
        "  for i in range(len(examples['attention_mask'])):\n",
        "    concatenated_examples['attention_mask']+=examples['attention_mask'][i]\n",
        "\n",
        "  # compute length of concatenated texts\n",
        "  total_length = len(concatenated_examples['input_ids'])\n",
        "\n",
        "  # We drop the last chunk if it's smaller than chunk_size\n",
        "  total_length = (total_length // chunk_size) * chunk_size\n",
        "\n",
        "  # Split by chunks of max_len\n",
        "  result = {\n",
        "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "\n",
        "  # Create a new labels column\n",
        "  result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "\n",
        "\n",
        "  result['input_ids'] = torch.tensor(result['input_ids'])\n",
        "  result['token_type_ids'] = torch.tensor(result['token_type_ids'])\n",
        "  result['attention_mask'] = torch.tensor(result['attention_mask'])\n",
        "  result['labels'] = torch.tensor(result['labels'])\n",
        "\n",
        "  \n",
        "  return result\n",
        "\n",
        "inputs = group_texts(result)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "We want to create our mask. \n",
        "Each token that is not a special token has a 15% chance of being masked.\n",
        "We don't want to mask [CLS], [SEP], or padding tokens. \n",
        "Correspond to the numbers 4, 5, and 1.\n",
        "\"\"\"\n",
        "\n",
        "rand = torch.rand(inputs['input_ids'].shape)\n",
        "\n",
        "mask_arr = (rand < 0.15) * (inputs['input_ids'] != 4) * (inputs['input_ids'] != 5)* (inputs['input_ids'] != 1)\n",
        "\n",
        "# masked tokens\n",
        "\n",
        "selection = []\n",
        "\n",
        "for i in range(mask_arr.shape[0]):\n",
        "  selection.append(\n",
        "    torch.flatten(mask_arr[i].nonzero()).tolist()\n",
        "  )\n",
        "\n",
        "for i in range(mask_arr.shape[0]):\n",
        "  inputs['input_ids'][i,selection[i]] = 0\n",
        "\n",
        "dataset = TweetsDataset(inputs)\n",
        "\n",
        "# pass dataset into DataLoader\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 32, shuffle=True)\n",
        "\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "model.train()\n",
        "\n",
        "from torch.optim import AdamW\n",
        "\n",
        "#optimizer\n",
        "optim = AdamW(model.parameters(), lr=5e-5, weight_decay = 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  loop = tqdm(dataloader, leave=True)\n",
        "  for batch in loop:\n",
        "    optim.zero_grad()\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels= batch['labels'].to(device)\n",
        "    \n",
        "\n",
        "    outputs = model(input_ids, attention_mask=attention_mask,\n",
        "                   lm_labels=labels)\n",
        "    loss, prediction_scores = outputs[:2]\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    loop.set_description(f'Epoch {epoch}')\n",
        "    loop.set_postfix(loss=loss.item())"
      ],
      "metadata": {
        "id": "uYffV5faZ1yf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a3f387-45fe-4b9f-bc0a-fc7a1a594b75"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]<ipython-input-3-3cabc11ba032>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "Epoch 0: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it, loss=11.6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save your model\n",
        "save_directory = \"/content/drive/MyDrive/myModels\"\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "model.save_pretrained(save_directory)"
      ],
      "metadata": {
        "id": "hDuzBY3hZ5mN"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}